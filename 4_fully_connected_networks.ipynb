{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81347816",
   "metadata": {},
   "source": [
    "# Neuronale Netze - Einführung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335204d6",
   "metadata": {},
   "source": [
    "Neuronale Netze sind Funktionen: sie haben eine Ein- und Ausgabe.\n",
    "Wir können sie z.B. \n",
    "- mit Bildern füttern, und sie sagen uns das Tier, das darauf abgebildet ist.\n",
    "- mit Daten über eine Schüler\\*in versorgen, und sie sagen uns die Abinote.\n",
    "- nach einem Gedicht fragen, und sie schreiben uns ein Haiku.\n",
    "\n",
    "Bevor sie dies machen können, müssen wir sie jedoch trainieren.\n",
    "Unsere neuronalen Netzwerke bestehen aus mehreren Schichten von Neuronen, die jeweils mit allen Neuronen der benachbarten Schichten verbunden sind.\n",
    "Wir überreichen die Eingabe an die erste Schicht (z.B. ein Pixel pro Neuron).\n",
    "In jeder Schicht erhält nun das Neuron die Ausgaben der Vorgänger, gewichtet sie, \n",
    "addiert sie und fügt noch einen Bias hinzu.\n",
    "\n",
    "Damit besteht ein Neuron aus einem Vektor $w$ an Gewichten und einer Zahl $b$ (Bias).\n",
    "Gegeben eine Eingabe $x$ (die Ausgabe der Neuronen des vorherigen Layers) führt es nun die Berechnung\n",
    "$w \\cdot x + b$\n",
    "aus.\n",
    "\n",
    "In PyTorch können wir ein Neuron mit der Funktion `torch.nn.Linear` erzeugen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = torch.nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58da08",
   "metadata": {},
   "source": [
    "Wir geben die Anzahl der Eingaben als erstes Argument an.\n",
    "Das zweite Argument gibt die Anzahl der Neuronen an, die wir in der Schicht erzeugen wollen.\n",
    "\n",
    "Die Gewichte und der Bias werden zufällig automatisch initialisiert, wir können sie aber auch überschreiben.\n",
    "Wir setzen $w = (1, 2)$ und $b = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec015e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([1.0,2.0])\n",
    "b = torch.tensor(3.0)\n",
    "\n",
    "neuron.weight.data = w\n",
    "neuron.bias.data = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b37f96",
   "metadata": {},
   "source": [
    "Was ist die Ausgabe des Neurons für $x = (3,2)$? Versuch es erst im Kopf bevor du die Zelle ausführst. Vergiss den Bias nicht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.0, 2.0])\n",
    "\n",
    "neuron(x).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa56f6b",
   "metadata": {},
   "source": [
    "**Aufgabe**\n",
    "- Erzeuge nun selbst einen Layer mit 10 Neuronen und je 3 Eingaben. \n",
    "- Gewichte und Bias brauchst du nicht anzupassen.\n",
    "- Generiere eine Eingabe $x$ und schicke sie durch den Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e56073",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = _ # Layer erzeugen\n",
    "x = _     # passende Eingabe (Werte egal)\n",
    "_         # x durch Layer schieben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31621bdf",
   "metadata": {},
   "source": [
    "Schau dir einmal die Gewichte und Bias an: wir haben separate Gewichte für jedes Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa736d5",
   "metadata": {},
   "source": [
    "Du kannst natürlich auch mehrere Eingaben gleichzeitig verarbeiten. Um uns Arbeit zu sparen können wir zufällige Eingaben erzeugen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rnd = torch.rand(3,5)\n",
    "x_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836017f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer(x_rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767679f",
   "metadata": {},
   "source": [
    "Ups, das ging schief. Lies die Fehlermeldung und versuche den Fehler zu korrigieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345eeb8",
   "metadata": {},
   "source": [
    "## Aktivierungsfunktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a04bf",
   "metadata": {},
   "source": [
    "Neuronen sollen die gesammelten Informationen erst ab einer gewissen Relevanz bzw. Stärke weitergeben.\n",
    "Dies erreichen wir durch eine Aktivierungsfunktion. Nachdem das Neuron seine Eingaben aufsummiert hat, wird diese angewandt. Die am häufigsten verwendete ist ReLU: sie gibt nur positive Werte weiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "three = torch.tensor([3])\n",
    "print(\"relu(-3) =\", relu(-three)[0])\n",
    "print(\"relu( 3) =\", relu(three)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385e41b",
   "metadata": {},
   "source": [
    "Versuche die ReLU-Funktion selbst zu implementieren:\n",
    "1. `my_single_relu` für einzelne Zahlen\n",
    "2. `my_tensor_relu` für torch Tensoren (nutze z.B. `torch.max`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_single_relu(x):\n",
    "    return _ # <-- Lösung hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b946894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tensor_relu(x):\n",
    "    return _ # <-- Lösung hier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9fe85",
   "metadata": {},
   "source": [
    "Teste die Funktionen auf verschiedenen Werten, es sollte immmer `True` ausgegeben werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_single = -32\n",
    "x_tensor = torch.tensor([32, -32])\n",
    "\n",
    "print(my_single_relu(x_single) == relu(torch.tensor(x_single)))\n",
    "print(my_tensor_relu(x_tensor) == relu(x_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a4204",
   "metadata": {},
   "source": [
    "Was ist die Ableitung der ReLU-Funktion, können wir sie für die Eingabe $0$ bestimmen?\n",
    "Argumentiere mit dem Differenzenquotienten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89fdbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b711dc3f",
   "metadata": {},
   "source": [
    "## Netzwerke schichten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2f834",
   "metadata": {},
   "source": [
    "Wir können nun ganz einfach die Layer zu einem Netzwerk schichten: immer Neuronen und Aktivierungsfunktionen im Wechsel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd30511",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 16),  # <-- input layer\n",
    "    torch.nn.ReLU(),# /      diese Zahlen müssen gleich sein\n",
    "    torch.nn.Linear(16, 16), # <-- hidden layer\n",
    "    torch.nn.ReLU(), # /     diese Zahlen müssen gleich sein\n",
    "    torch.nn.Linear(16, 16), # <-- hidden layer\n",
    "    torch.nn.ReLU(), # /     diese Zahlen müssen gleich sein\n",
    "    torch.nn.Linear(16, 1)   # <-- output layer\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e414c",
   "metadata": {},
   "source": [
    "Wir können nun eine Eingabe $x = 2$ durch das Netzwerk schicken, jedoch müssen wir sie dafür zuvor in einen Tensor packen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407329a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([2.0])\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cb809",
   "metadata": {},
   "source": [
    "Verdopple nun den vorletzten Layer auf eine Größe von 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_32 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 1)\n",
    ")\n",
    "net_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb9cc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Korrekt!\" if net_32[4].in_features == 32 else \"Falsche Anzahl Neuronen\")\n",
    "\n",
    "x = torch.tensor([2.0])\n",
    "net_32(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23410f6",
   "metadata": {},
   "source": [
    "## Neuronale Netze Trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67989243",
   "metadata": {},
   "source": [
    "Zu Beginn haben wir gesagt, dass neuronale Netze Funktionen sind, sie können sogar jede beliebige Funktion darstellen. Dafür müssen wir jedoch die Gewichte richtig setzen.\n",
    "Die Gewichte richtig einzustellen ist von Hand jedoch schnell mühsam, bereits unser kleines Netz aus dem letzten Kapitel hat bereits zu viele Parameter (Gewichte inkl. Bias)! Wie viele eigentlich? (Tipp: Jedes Neuron hat ein Gewicht pro Eingabe, plus den Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72810cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weights = _ # <-- Lösung eintragen\n",
    "num_weights == sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e0fae3",
   "metadata": {},
   "source": [
    "Wir brauchen also eine automatische Möglichkeit die Parameter zu optimieren (wir nennen das \"trainieren\").\n",
    "\n",
    "Die Grundidee ist, dass wir Eingaben, für die wir das Ergebnis bereits kennen durch das Netzwerk schicken, und das Ergebnis des neuronalen Netzes mit dem erwarteten Wert vergleichen. Diese Daten heißen Trainingsdaten, ein Beispiel hier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d11e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.load('data/1d_dataset_train.pt')\n",
    "x_train = data_train['x']\n",
    "y_train = data_train['y']\n",
    "plt.plot(x_train, y_train, '.', markersize=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7df0d8",
   "metadata": {},
   "source": [
    "Nun stellen wir die Gewichte ein (dazu später mehr), und prüfen mittels der Testdaten, wie nah wir schon an der Funktion sind. Zum Vergleich in blau die aktuellen (schlechten) Ergebnisse unseres Netzwerks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb571278",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = torch.load('data/1d_dataset_test.pt')\n",
    "x_test = data_test['x']\n",
    "y_test = data_test['y']\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_test = net(x_test)\n",
    "\n",
    "plt.plot(x_test, y_pred_test, '.', markersize=0.1)\n",
    "plt.plot(x_test, y_test, '.', markersize=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a61751c",
   "metadata": {},
   "source": [
    "### Fehlerfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bc7cc",
   "metadata": {},
   "source": [
    "Wir sehen z.B. für $x \\approx 0.55$, dass wir ca. $0.82$ erwarten, unser Netzwerk jedoch ca. $0$ vorhersagt. Indem du `i` änderst, kannst andere Datenpunkte auswählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398f38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(\"Eingabe: \", x_test[i].item())\n",
    "print(\"Erwartete Ausgabe: \", y_test[i].item())\n",
    "print(\"Vorhersage Netzwerk: \", y_pred_test[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4596b",
   "metadata": {},
   "source": [
    "Um auszudrücken, wie gut unsere aktuellen Vorhersagen sind, wollen wir den Fehler zwischen erwarteter Ausgabe und der Vorhersage berechnen. Dieser ist einfach der durchschnittliche Fehler gemittelt über alle Testdaten.\n",
    "\n",
    "Für einen einzelnen Datenpunkt berechnen wir den Fehler als\n",
    "$(Y^i_{\\textit{test}} - Y^i_{\\textit{pred}})^2$ (Squared Error).\n",
    "Man könnte erwarten, dass der Fehler als $\\|Y^i_{\\textit{test}} - Y^i_{\\textit{pred}}\\|$ definiert sein sollte, aber damit lässt sich schlechter arbeiten, deswegen quadrieren wir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca8298",
   "metadata": {},
   "source": [
    "Implementiere den Squared Error für einen Datenpunkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_err(x,y):\n",
    "    return _ # <--- Lösung hier\n",
    "\n",
    "y_test_i = 3.0\n",
    "y_pred_i = 6.0\n",
    "\n",
    "# Sollte 9.0 zurückgeben\n",
    "squared_err(y_test_i, y_pred_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587d112",
   "metadata": {},
   "source": [
    "Programmiere nun den Mean Squared Error (MSE) als durchschnittlichen Fehler über alle Datenpunkte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y):\n",
    "    # Lösung hier\n",
    "    return _\n",
    "\n",
    "# Sollte 5.0 zurückgeben\n",
    "mse_x = torch.tensor([3.0, 4.0])\n",
    "mse_y = torch.tensor([6.0, 5.0])\n",
    "\n",
    "mse(mse_x, mse_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f5349",
   "metadata": {},
   "source": [
    "Natürlich ist der MSE in PyTorch bereits eingebaut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ef9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_mse = torch.nn.MSELoss()\n",
    "\n",
    "torch_mse(mse_x, mse_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0cdf9",
   "metadata": {},
   "source": [
    "Wie du siehst, heißt die Funktion `MSELoss`. *Loss* bezeichnet in der Optimierung für den Fehler gegenüber der optimalen Lösung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26b9e9",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da554c6",
   "metadata": {},
   "source": [
    "Wir wollen den Fehler optimieren, d.h. möglichst auf $0$ drücken. Dann sollte sich die blaue Kurve den roten Datenpunkten annähern.\n",
    "Um das hinzubekommen können wir die Gewichte des neuronalen Netzwerks anpassen, die in jedem Neuron gespeichert sind.\n",
    "\n",
    "Wir wählen zuerst ein paar zufällige Trainingsdaten (Minibatch) aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ind = torch.randint(len(x_train), (16,))\n",
    "x_batch = x_train[batch_ind, :]\n",
    "y_batch = y_train[batch_ind, :]\n",
    "print(batch_ind)\n",
    "print(x_batch)\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa62e99",
   "metadata": {},
   "source": [
    "Diese schicken wir durch das Netzwerk und bestimmen damit unsere Vorhersagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ebb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net(x_batch)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbc82e",
   "metadata": {},
   "source": [
    "Für jede Vorhersage wissen wir auch, was die korrekte Ausgabe wäre und können daher den Loss berechnen.\n",
    "Diese Phase ist der Forward-Pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d772275",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch_mse(y_pred, y_batch)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef8bf2",
   "metadata": {},
   "source": [
    "Nun bestimmen wir mittels Backpropagation, wie wir die Gewichte anpassen müssen, um den Loss ein klein wenig geringer zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f505066",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d91a8",
   "metadata": {},
   "source": [
    "Um zu bestimmen, wie genau die Gewichte angepasst werden sollen, benötigen wir einen Optimizer. Wir verwenden den sehr bekannten Algorithmus *Adam*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "print(\"Einige Parameter vor dem Schritt:\")\n",
    "print(list(net.parameters())[0].data)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\nEinige Parameter nach dem Schritt:\")\n",
    "print(list(net.parameters())[0].data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c82828",
   "metadata": {},
   "source": [
    "Nach jedem Schritt müssen die Gradienten mittels `optimizer.zero_grad()` zurückgesetzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a47ef",
   "metadata": {},
   "source": [
    "**Aufgabe** Implementiere den Trainingsalgorithmus und trainiere dein Netzwerk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "for it in tqdm(range(max_iter)):\n",
    "    # Erzeuge Mini-Batch\n",
    "    \n",
    "    # Vorhersage des Netzwerks\n",
    "    \n",
    "    # Loss berechnen\n",
    "\n",
    "    # Backpropagation\n",
    "    \n",
    "    # Optimizer step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528582d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91c3b9d2",
   "metadata": {},
   "source": [
    "**Aufgabe** Berechne die Vorhersagen des Netzwerkes und bestimme den Loss auf dem Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55819fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_test = _ # Lösung hier\n",
    "    err = _ # Lösung hier\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b4ad8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a57dacb1",
   "metadata": {},
   "source": [
    "**Aufgabe** Erweitere deinen Trainingsalgorithmus, sodass schon während des Trainings die Performance des Netzes in regelmäßigen Abständen bestimmt wird und der Loss gespeichert wird, sodass du ihn nach dem Training plotten kannst.\n",
    "Passe außerdem deine Netzwerkarchitektur und die Trainingsparameter an, sodass du bessere Ergebnisse bekommst.\n",
    "\n",
    "Schritte:\n",
    "1. Kopiere Algorithmus\n",
    "2. Jeden 100ten Schritt: berechne Loss auf **Test**daten, gib ihn aus\n",
    "3. Jeden 100ten Schritt: plotte Vorhersagen auf den **Test**daten vs. erwartete Ausgaben\n",
    "4. In jedem Schritt: Speichere Loss auf **Trainings**daten in Liste.\n",
    "5. Passe Netzwerkarchitektur + Parameter an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d520b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss während des Trainings plotten\n",
    "plt.plot(_) # Trage hier dein Loss-Array ein\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cba1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss auf Testdaten berechnen\n",
    "\n",
    "# Ergebnisse grafisch darstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22233b62",
   "metadata": {},
   "source": [
    "## Ziffern klassifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed6611",
   "metadata": {},
   "source": [
    "### Aufgabe 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5172aad",
   "metadata": {},
   "source": [
    "Als nächstes widmen wir uns der Klassifikation von Ziffern. Wir verwenden hierfür den MNIST-Datensatz, den man sich über die Funktion [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) herunterladen kann. Außerdem kannst du mit der Funktion direkt Transformationen auf dem Datensatz ausführen. Wir wollen die Bilder direkt mit [`torchvision.transforms.ToTensor`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor) zu Tensoren konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('data/', train=True, \n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "mnist_test = torchvision.datasets.MNIST('data/', train=False, \n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9b57b",
   "metadata": {},
   "source": [
    "Einige Zifferen als Beispiele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b198850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.imshow(torch.reshape(mnist_train[i][0], (28, 28)))\n",
    "    plt.show()\n",
    "\n",
    "    print(mnist_train[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06993d",
   "metadata": {},
   "source": [
    "Beim Training des letzten Modells haben wir die Mini-Batches manuell erzeugt. Allerdings gibt es die Funktion [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), die einem diese Aufgabe abnimmt.\n",
    "\n",
    "**Aufgabe** Erzeuge einen solchen `DataLoader` für das Trainings- und Testset mit Batch-Größe 16. Achte darauf, dass (nur) die Trainingsdaten gemischt werden müssen. Schaue dafür ggf. in die Dokumentation/such im Internet/bei ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dfc49",
   "metadata": {},
   "source": [
    "Der Aufbau der neuronalen Netzes zur Klassifikation der Ziffern ist ähnlich dem im vorherigen Problem, allerdings müssen wir einige Dinge beachten.\n",
    "\n",
    "Da es sich um Bilder handelt, müssen wir diese erst in Vektoren umwandeln. Das ist mit der Funktion [`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) möglich, die eine Schicht des neuronalen Netzwerks erzeugt.\n",
    "Außerdem muss das Netzwerk jetzt nicht mehr eine sondern 10 Zahlen zurück geben.\n",
    "Jede stellt die Wahrscheinlichkeit dar, dass die konkrete Eingabe die jeweilige Ziffer darstellt.\n",
    "\n",
    "**Aufgabe** Implementiere ein solches Netz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e839b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = _ # Lösung hier\n",
    "\n",
    "net(torch.zeros(1, 28, 28)) # Test, sollte keinen Fehler liefern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bcac0",
   "metadata": {},
   "source": [
    "**Aufgabe** Erstelle außerdem den Adam-`Optimizer` und den für Klassifikation benötigten [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = _\n",
    "optimizer = _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2bd4b",
   "metadata": {},
   "source": [
    "Um das Netzwerk zu trainieren, führen wir eine `for`-Schleife über den `DataLoader` aus. Damit iterieren wir einmal durch den gesamten Datensatz, was als eine Epoche bezeichnet wird. Mit einer zweiten `for`-Schleife können wir mehrere solcher Epochen ausführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db771cc",
   "metadata": {},
   "source": [
    "**Aufgabe** Ergänze den Trainingsalgorithmus und trainiere dein Modell. Plotte anschließen den Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1881f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 5\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        # Vorhersage des Netzwerks\n",
    "\n",
    "        # Loss berechnen\n",
    "        \n",
    "        # Backpropagation\n",
    "        \n",
    "\n",
    "        # Optimizer step\n",
    "        \n",
    "        # Speichere Loss in loss_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c635794",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965482e7",
   "metadata": {},
   "source": [
    "**Aufgabe** Berechne die Genauigkeit deines Netzwerkes, also wie viele Bilder richtig klassifiziert werden, auf dem Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_correct = 0\n",
    "sum_imgs = 0\n",
    "\n",
    "for x_batch, y_batch in tqdm(test_loader):\n",
    "    # Vorhersage des Netzes ohne Gradientenberechnung\n",
    "    with torch.no_grad():\n",
    "        y_pred = _\n",
    "    \n",
    "    # Vorhergesagtes Label\n",
    "    y_pred = _\n",
    "        \n",
    "    # Anzahl der Bilder aktualisieren\n",
    "    sum_imgs += _\n",
    "    \n",
    "    # Anzahl der korrekt klassifizierten Bilder\n",
    "    sum_correct += _\n",
    "\n",
    "# Accuracy berechnen und ausgeben\n",
    "accuracy = _\n",
    "print('Accuracy auf dem Testset: ', str(accuracy * 100) + \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376999e",
   "metadata": {},
   "source": [
    "**Aufgabe** Wiederhole das Training und berechne die Genauigkeit diesmal nach jeder Epoche. Verbessere außerdem deine Netzwerkarchitektur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Du kannst deinen Code aus den vorherigen Aufgaben nutzen und anpassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d2ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8f508d",
   "metadata": {},
   "source": [
    "# Bonus: Backpropagation selbst implementieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131507c",
   "metadata": {},
   "source": [
    "Einige imports zu Beginn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cfc56",
   "metadata": {},
   "source": [
    "Wir definieren eine einfache Funktion $f(x) := 3x^2 - 4x + 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8466f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return 3*x**2 - 4*x + 5\n",
    "\n",
    "print(\"f(3) =\", f(3.0))\n",
    "\n",
    "\n",
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c7e69",
   "metadata": {},
   "source": [
    "Bestimme die Ableitung bei $x = 3$ mit dem Differenzenquotienten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac439ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "x = 3.0\n",
    "dx = _ # Lösung hier\n",
    "dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a14a8",
   "metadata": {},
   "source": [
    "Für zusammengesetzte Ausdrücke funktioniert das Ganze ebenfalls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33631cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingaben\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d1 = a*b + c\n",
    "c += h\n",
    "d2 = a*b + c\n",
    "\n",
    "print('d1', d1)\n",
    "print('d2', d2)\n",
    "print('slope', (d2 - d1)/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e59df",
   "metadata": {},
   "source": [
    "Wir wollen nun automatisch die Gradienten für solche Ausdrücke bestimmen.\n",
    "Dafür definieren wir eine Klasse `Value`, die diese repräsentiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad852bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "    self._prev = set(_children)\n",
    "    self.grad = 0.0\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "\n",
    "  def __add__(self, other):\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    return out\n",
    "  \n",
    "  def relu(self):\n",
    "    x = self.data\n",
    "    t = max(0,x)\n",
    "    out = Value(t, (self, ), 'relu')\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57677ea0",
   "metadata": {},
   "source": [
    "Wir können nun einen ähnlichen Ausdruck erzeugen und auswerten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046cd34",
   "metadata": {},
   "source": [
    "Um die Ausdrücke anschaulicher zu machen bauen wir eine Funktion, die diese zeichnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc98fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} graphviz\n",
    "!{sys.executable} -m pip install graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f929c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de6c90",
   "metadata": {},
   "source": [
    "Versuche mit der `Value`-Klasse einen Fully Connected Layer mit 2 Inputs, einem Output und ReLU-Aktivierung zu definieren. Gibt den Knoten zufällige Werte und aussagekräftige Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978595b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = _\n",
    "x2 = _\n",
    "# weights w1,w2\n",
    "w1 = _\n",
    "w2 = _\n",
    "# bias of the neuron\n",
    "b = _\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = _\n",
    "x2w2 = _\n",
    "x1w1x2w2 = _\n",
    "n = _; n.label = 'n'\n",
    "o = _; o.label = 'o'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca72fb",
   "metadata": {},
   "source": [
    "Zeichne dein Netzwerk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35b3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd9545ff",
   "metadata": {},
   "source": [
    "Wir berechnen die Ableitung jedes Knotens rückwärts, also von rechts nach links.\n",
    "Angenommen, wir wissen bereits, wie sich die Ausgabe des Ausdrucks abhängig von $c = a + b$ ändert. Jetzt können wir auch bestimmen, wie sich die Ausgabe abhängig von $a$ oder $b$ ändert, ganz einfach mittels der Kettenregel:\n",
    "Sei z.B. der Gradient von c $2$, wenn wir nun $c$ um $h = 0.0001$ erhöhen, erhöht sich auch die Ausgabe um ca. $2 * h$.\n",
    "Wie erhöht sich die Ausgabe, wenn wir $a$ um $h$ erhöhen? Genau, auch um ca. $2 * h$, wir können also einfach den Gradienten von $c$ übernehmen.\n",
    "\n",
    "Füge dies in der Klasse unten hinzu: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "    self._prev = set(_children)\n",
    "    self.grad = 0.0\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "\n",
    "  def __add__(self, other):\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad = _ # übernimm out.grad\n",
    "      other.grad = _ # übernimm out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    return out\n",
    "  \n",
    "  def relu(self):\n",
    "    x = self.data\n",
    "    t = max(0,x)\n",
    "    out = Value(t, (self, ), 'relu')\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17528c",
   "metadata": {},
   "source": [
    "Etwas komplizierter ist es bei der Multiplikation. Für $c = a * b$ und $a = -2$, Gradient von c = $3$ gilt, wenn wir $b$ um $h$ erhöhen, ändert sich die Ausgabe um $-2 * 3 * h = 6 * h$. Wir müssen also mit `a.data` multiplizieren.\n",
    "\n",
    "Füge dies und die Ableitung von ReLU in der Klasse unten hinzu: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696714e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self.grad = 0.0\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "\n",
    "  def __add__(self, other):\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad = _ # übernimm out.grad\n",
    "      other.grad = _ # übernimm out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += _\n",
    "      other.grad += _\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def relu(self):\n",
    "    x = self.data\n",
    "    t = max(0,x)\n",
    "    out = Value(t, (self, ), 'relu')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad = _\n",
    "    \n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63a2bf",
   "metadata": {},
   "source": [
    "Setze nun den Gradienten von $o = 0$ und ruf dann in der richtigen Reihenfolge (von rechts nach links) die Funktion `_backward` auf den Knoten auf. Überprüfe das Ergebnis auf Korrektheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907e1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "270107c4",
   "metadata": {},
   "source": [
    "Um nicht jedes Mal von Hand die `_backward`-Funktion aufrufen zu müssen, schreiben wir eine Funktion, die die richtige Reigenfolge berechnet und dann überall `_backward` aufruft.\n",
    "Die Reihenfolge zu definieren geht so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead82be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topo = []\n",
    "visited = set()\n",
    "def build_topo(v):\n",
    "  if v not in visited:\n",
    "    _ # füge v zu visited hinzu\n",
    "    for child in v._prev:\n",
    "        _ # rekursiver Aufruf\n",
    "    _ # füge v an topo an\n",
    "build_topo(o)\n",
    "list(reversed(topo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802f446",
   "metadata": {},
   "source": [
    "Implementiere die Funktion `backward`, sodass sie die Topographie berechnet, dann den Gradienten des aktuellen Knoten auf $1.0$ setzt und schließlich überall `_backward()` aufruft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self.grad = 0.0\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "\n",
    "  def __add__(self, other):\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad = 1.0 * out.grad # übernimm out.grad\n",
    "      other.grad = 1.0 * out.grad # übernimm out.grad\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    return out\n",
    "  \n",
    "  def relu(self):\n",
    "    x = self.data\n",
    "    t = max(0,x)\n",
    "    out = Value(t, (self, ), 'relu')\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def backward(self):\n",
    "    _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36dd56",
   "metadata": {},
   "source": [
    "Irgendetwas geht noch schief:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a   ; b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449c9aa",
   "metadata": {},
   "source": [
    "Versuche den Fehler zu finden und zu korrigieren.\n",
    "Tipp: `=` $\\mapsto$ `+=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e9ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb50ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
