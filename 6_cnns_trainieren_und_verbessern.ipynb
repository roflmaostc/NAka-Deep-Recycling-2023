{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0b26bc",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65290db0",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks für Bildklassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb837d",
   "metadata": {},
   "source": [
    "Zu Beginn wollen wir das Klassifizieren der Ziffern des MNIST Datensatzes aufgreifen, was wir bereits mit Fully Connected Networks (FCNs) durchgeführt haben. Dazu bereiten wir den Datensatz, die `DataLoader` und die Loss-Funktion vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4226fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('data/', train=True, \n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "mnist_test = torchvision.datasets.MNIST('data/', train=False, \n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=16)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1113f2",
   "metadata": {},
   "source": [
    "### Aufgabe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87523d",
   "metadata": {},
   "source": [
    "Die wichtigsten Bestandteile eines Convolutional Neural Networks (CNNs) sind die [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) Layer. Die Faltungskernel haben üblicherweise die Größe $3\\times 3$. Im Verlaufe des Netzwerk wird das Bild immer weiter downgesampelt, indem beispielsweise bei der Faltung das Stride auf $2$ gesetzt wird oder Pooling Layer wie [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) verwendet werden. Nach jedem Downsampling wird die Anzahl der Channels um einen Faktor von $2$ erhöht, um die Zahl der Neuronen nicht zu stark zu reduzieren.\n",
    "\n",
    "Das letzte Layer ist fully connected, sodass man die Wahrscheinlichkeiten für die gewünschte Anzahl an Klassen erhält. Da ein solches Layer allerdings ein Batch von Vektoren erwartet, muss man die Features vorher in Vektoren umwandeln. Eine Möglichkeit dies zu tun haben wir uns in der Übung zu FCNs angeschaut, eine andere ist [`torch.nn.AvgPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e2db5",
   "metadata": {},
   "source": [
    "Baue ein Netzwerk, was aus $5$-$10$ Layern besteht und zwei Downsampling Operationen enthält. Überprüfe, ob die Dimensionen der Layer zusammen passen, indem du ein Batch mit deinem Netzwerk verarbeitest.\n",
    "\n",
    "Füge nach den Convolution-Layern ReLU-Layer ein. Du brauchst am Übergang zu den Fully Connected Layern die Operation  `torch.nn.Flatten()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55712974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.B. torch.nn.AvgPool2d(2, 2)\n",
    "?torch.nn.AvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.B. torch.nn.Conv2d(1, 8, 3, padding=1)\n",
    "# Argumente: in_channels, out_channels, kernel_size\n",
    "?torch.nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = _\n",
    "\n",
    "x = mnist_train[0][0].reshape(1, 1, 28, 28)\n",
    "out = net(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32128790",
   "metadata": {},
   "source": [
    "### Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ff3d8",
   "metadata": {},
   "source": [
    "Trainiere dein Netzwerk und werte die Genauigkeit auf dem Testset aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39184c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    sum_correct = 0\n",
    "    sum_imgs = 0\n",
    "\n",
    "    for x_batch, y_batch in loader:\n",
    "        # Vorhersage des Netzes ohne Gradientenberechnung\n",
    "\n",
    "        # Vorhergesagtes Label\n",
    "        \n",
    "        # Anzahl der Bilder updaten\n",
    "        \n",
    "        # Anzahl der korrekt klassifizierten Bilder\n",
    "        \n",
    "    # Accuracy berechnen und zurückgeben\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Erstellen des Optimizers\n",
    "\n",
    "# Training\n",
    "n_epoch = 6\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        # Vorhersage des Netzwerks\n",
    "\n",
    "        # Loss berechnen\n",
    "        \n",
    "        # Backpropagation\n",
    "        \n",
    "        # Optimizer step + Gradienten löschen\n",
    "\n",
    "        # Loss speichern\n",
    "    \n",
    "    # Accuracy bestimmen mittels evaluate + test_loader\n",
    "    acc = _\n",
    "    print('Accuracy nach %i Epoche(n): %.2f%%' % (ep + 1, acc * 100))\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d3e3f",
   "metadata": {},
   "source": [
    "## Neuronale Netze evaluieren und verbessern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b5082",
   "metadata": {},
   "source": [
    "### Aufgabe 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d2e2b",
   "metadata": {},
   "source": [
    "Die Confusion Matrix ist ein hilfreiches Tool, um die Fehler, die ein Netzwerk macht, zu verstehen. Sie kann man mit der Funktion `sklearn.metrics.confusion_matrix` ermitteln."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fcf16",
   "metadata": {},
   "source": [
    "Bestimme die Confusion Matrix und stelle sie grafisch dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293f291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm(test_loader):\n",
    "        y_pred = _\n",
    "        y_pred_label = torch.argmax(y_pred, dim=-1)\n",
    "        \n",
    "        labels.extend(y_batch.numpy())\n",
    "        predictions.extend(y_pred_label.numpy())\n",
    "        \n",
    "\n",
    "# Confusion Matrix bestimmen\n",
    "conf_mat = _\n",
    "\n",
    "# Grafische Darstellung\n",
    "classes = list(range(10))\n",
    "conf_mat_pd = pd.DataFrame(conf_mat, classes, classes)\n",
    "plt.figure(figsize=(15, 9))\n",
    "sns.heatmap(conf_mat_pd, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2579057",
   "metadata": {},
   "source": [
    "### Aufgabe 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566169d6",
   "metadata": {},
   "source": [
    "Ziffern zu klassifizieren ist ein relativ einfaches Problem und man kann mit relativ einfachen Modellen bereits eine Genauigkeit von $99\\%$ erreichen. Deswegen wollen wir jetzt mit dem komplexeren Datensatz CIFAR10 arbeiten, der Farbbilder der Größe $32\\times 32$ Pixel aus $10$ Klassen wie Frosch und Schiff beinhaltet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525f657",
   "metadata": {},
   "source": [
    "Wir laden den Datensatz mit [`torchvision.datasets.CIFAR10`](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html) und visualisieren einige Bilder aus verschiedenen Klassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ba918",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_ = torchvision.datasets.CIFAR10('data/', train=True, \n",
    "                                             transform=torchvision.transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "cifar10_test = torchvision.datasets.CIFAR10('data/', train=False, \n",
    "                                            transform=torchvision.transforms.ToTensor(),\n",
    "                                            download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d86fe",
   "metadata": {},
   "source": [
    "### Aufgabe 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3eaee",
   "metadata": {},
   "source": [
    "Teile den Trainingsdatensatz und ein Trainings- und einen Validierungsdatensatz auf, sodass zur Validierung etwa $10\\%$ der ursprünglichen Trainingsdaten verwendet werden. Erstelle `DataLoader` für den Trainings-, Validierungs- und Testdatensatz, nur der Trainingloader soll geshufflet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train, cifar10_val = torch.utils.data.random_split(cifar10_train_, \n",
    "                                                           [_, _]) # Trainingsanzahl, Testanzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=16, shuffle=True)\n",
    "val_loader = _\n",
    "test_loader = _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34635a",
   "metadata": {},
   "source": [
    "### Aufgabe 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2d199",
   "metadata": {},
   "source": [
    "Konstruiere ein CNN für den CIFAR10-Datensatz. Da die Bilder größer sind als die von MNIST, solltest du $3$ Downsampling Operationen einbauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb63fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = # ???\n",
    "\n",
    "x = cifar10_train[0][0].reshape(1, 3, 32, 32)\n",
    "out = net(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a922907",
   "metadata": {},
   "source": [
    "### Aufgabe 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70377831",
   "metadata": {},
   "source": [
    "Trainiere dein CNN. Bestimme dabei die Genauigkeit nach jeder Epoche und stelle sie, den Loss, sowie die Confusion Matrix nach dem Training grafisch dar. \n",
    "\n",
    "Verwende bei der Evaluierung auschließlich den Validierungsdatensatz. Den Testdatensatz wollen wir erst ganz am Ende benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036fcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd8a819",
   "metadata": {},
   "source": [
    "### Aufgabe 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8d8a8",
   "metadata": {},
   "source": [
    "Versuche die Performance deines Modells zu verbessern, indem du beispielsweise die Architektur optimierst, Regularisierung oder Data Augmentation verwendest und Learning Rate Decay benutzt.\n",
    "\n",
    "Beachte, dass Batch Normalization und Dropout Layer sich während des Trainings und der Evaluierung unterschiedlich verhalten haben, weswegen du deinem CNN explizit mit `net.train()` oder `net.eval()` mitteilen musst, in welcher Phase es sich befindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d3db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b5c8df1",
   "metadata": {},
   "source": [
    "### Aufgabe 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bed74a",
   "metadata": {},
   "source": [
    "Bestimme nun die Genauigkeit und die Confusion Matrix auf dem Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1e1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
